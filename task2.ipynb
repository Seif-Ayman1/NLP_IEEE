{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Natural Language Processing (NLP) with Arabic and English Languages\n",
    "\n",
    "In this notebook, we will explore various preprocessing techniques used in Natural Language Processing (NLP) for both Arabic and English languages. The focus will be on techniques such as diacritization, morphological analysis, stemming, lemmatization, and sentiment analysis preprocessing. We will utilize libraries such as NLTK, spaCy, PyArabic, Farasa, and Hugging Face Transformers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Table of Contents\n",
    "1. [Some libiraries](#some-libiraries)  \n",
    "   - [Nltk](#nltk)\n",
    "   - [RegEx](#regex)\n",
    "   - [Hugging Face](#hugging-face)\n",
    "   - [spaCy](#spacy)\n",
    "   - [PyArabic](#pyarabic)\n",
    "   - [Farasa](#farasa)\n",
    "2. [Setup](#setup)\n",
    "3. [Arabic Language Preprocessing](#arabic-language-preprocessing)\n",
    "   - [Diacritization](#diacritization)\n",
    "   - [Morphological Analysis](#morphological-analysis)\n",
    "   - [Dialect Handling](#dialect-handling)\n",
    "4. [English Language Preprocessing](#english-language-preprocessing)\n",
    "   - [Stemming](#stemming)\n",
    "   - [Lemmatization](#lemmatization)\n",
    "   - [Handling Abbreviations](#handling-abbreviations)\n",
    "5. [Advanced Text Handling](#advanced-text-handling)\n",
    "   - [Multilingual Processing](#multilingual-processing)\n",
    "   - [Sentiment Analysis Preprocessing](#sentiment-analysis-preprocessing)\n",
    "6. [Conclusion](#conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Libraries and Tools\n",
    "\n",
    "\"\"\"\n",
    "1. NLTK (Natural Language Toolkit):\n",
    "   - A comprehensive Python library for natural language processing (NLP).\n",
    "   - Provides tools for tokenization, stemming, lemmatization, and syntactic parsing.\n",
    "   - Widely used for academic research and beginner NLP tasks.\n",
    "\n",
    "2. RegEx (Regular Expressions):\n",
    "   - A powerful tool for pattern matching and text manipulation.\n",
    "   - Enables tasks like searching, replacing, and extracting specific patterns in text.\n",
    "   - Essential for preprocessing and cleaning raw data.\n",
    "\n",
    "3. Hugging Face:\n",
    "   - A leading library for state-of-the-art transformer-based NLP models like BERT, GPT, and T5.\n",
    "   - Simplifies tasks like text classification, question answering, and text generation.\n",
    "   - Includes the `transformers` and `datasets` libraries for seamless ML workflows.\n",
    "\n",
    "4. spaCy:\n",
    "   - An industrial-strength NLP library with high-performance features.\n",
    "   - Supports tokenization, named entity recognition (NER), and dependency parsing.\n",
    "   - Designed for production use with a focus on speed and efficiency.\n",
    "\n",
    "5. PyArabic:\n",
    "   - A Python library tailored for Arabic text processing.\n",
    "   - Provides utilities for diacritization, text normalization, and linguistic analysis.\n",
    "   - Ideal for preprocessing Arabic-language datasets.\n",
    "\n",
    "6. Farasa:\n",
    "   - A suite of NLP tools specifically built for Arabic.\n",
    "   - Includes features like part-of-speech tagging, tokenization, and named entity recognition.\n",
    "   - Known for its accuracy in handling complex Arabic linguistic structures.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Setup <a name=\"setup\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "# Install necessary libraries if not already installed\n",
    "!pip install nltk spacy pyarabic farasa transformers\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing necessary libraries\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import re\n",
    "import spacy\n",
    "from pyarabic.araby import diacritize\n",
    "from farasa.pos import FarasaPOSTagger\n",
    "from transformers import pipeline\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Arabic Language Preprocessing <a name=\"arabic-language-preprocessing\"></a>\n",
    "\n",
    "### Diacritization <a name=\"diacritization\"></a>\n",
    "\n",
    "Diacritization is the process of adding diacritics (vowel markings) to Arabic text. This is crucial for accurate pronunciation and understanding. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Arabic text (without diacritics)\n",
    "arabic_text = \"لقد قلت لأبي : أريد أن أذهب إلى الحديقه\"\n",
    "\n",
    "# Diacritization using PyArabic\n",
    "diacritized_text = diacritize(arabic_text)\n",
    "print(\"Diacritized Text:\", diacritized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Explanation:**  \n",
    "Diacritization helps in understanding the correct pronunciation of Arabic words. The `pyarabic` library provides an easy way to add diacritics to Arabic text, making it useful for text-to-speech systems and linguistic analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphological Analysis <a name=\"morphological-analysis\"></a>\n",
    "\n",
    "Morphological analysis involves studying the structure of words in Arabic, which is rich in morphology.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of morphological analysis using Farasa\n",
    "farasa_pos_tagger = FarasaPOSTagger(interactive=True)\n",
    "pos_tags = farasa_pos_tagger.tag(arabic_text)\n",
    "print(\"Morphological Analysis:\", pos_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**  \n",
    "Farasa's POS tagger provides morphological analysis, which is essential for understanding the grammatical structure of Arabic text. This helps in various NLP tasks such as information retrieval and machine translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dialect Handling <a name=\"dialect-handling\"></a>\n",
    "\n",
    "Dialect handling is essential for Arabic as it has many regional variations. In this notebook, we will not implement a specific dialect handling technique, but it's important to consider it in practical applications.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define the Dialect Mapping\n",
    "\n",
    "We'll create a dictionary that maps some words from Egyptian Arabic to their MSA forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dialect mapping from Egyptian Arabic to Modern Standard Arabic (MSA)\n",
    "dialect_mapping = {\n",
    "    \"عايز\": \"أريد\",  # \"want\" in Egyptian Arabic\n",
    "    \"فين\": \"أين\",    # \"where\" in Egyptian Arabic\n",
    "    \"هاروح\": \"سأذهب\",  # \"I will go\" in Egyptian Arabic\n",
    "    \"كده\": \"بهذه الطريقة\",  # \"like this\" in Egyptian Arabic\n",
    "    \"حاجة\": \"شيء\"    # \"thing\" in Egyptian Arabic\n",
    "}\n",
    "\n",
    "# Function to convert Egyptian Arabic to MSA\n",
    "def convert_to_msa(text):\n",
    "    for egyptian_word, msa_word in dialect_mapping.items():\n",
    "        text = text.replace(egyptian_word, msa_word)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Use the Function with Sample Text\n",
    "\n",
    "Now we'll use the function on a sample sentence in Egyptian Arabic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text in Egyptian Arabic\n",
    "egyptian_arabic_text = \"أنا عايز أروح فين؟\"\n",
    "\n",
    "# Convert to Modern Standard Arabic\n",
    "msa_text = convert_to_msa(egyptian_arabic_text)\n",
    "print(\"Converted to MSA:\", msa_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance\n",
    "\n",
    "This technique is important in NLP applications to ensure that models trained on MSA can understand and process dialectal Arabic input. The complexity of dialects can vary, so more advanced methods may involve leveraging machine learning models or rule-based systems to handle a wider variety of dialectal nuances. \n",
    "\n",
    "This example is a simple illustration and can be extended with more comprehensive mappings, including phrases and more dialects, depending on the application's needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## English Language Preprocessing <a name=\"english-language-preprocessing\"></a>\n",
    "\n",
    "### Stemming <a name=\"stemming\"></a>\n",
    "\n",
    "Stemming reduces words to their root form. For example, \"running\" becomes \"run\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming using NLTK\n",
    "stemmer = PorterStemmer()\n",
    "english_text = \"running runner runs\"\n",
    "tokens = nltk.word_tokenize(english_text)\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**  \n",
    "Stemming is a simple and efficient way to reduce words to their base forms, which can help improve the performance of search engines and text classification algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization <a name=\"lemmatization\"></a>\n",
    "\n",
    "Lemmatization is similar to stemming but considers the context and converts words to their base form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization using NLTK\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**  \n",
    "Lemmatization provides a more accurate base form of words compared to stemming, which is vital for tasks requiring more nuanced understanding, such as sentiment analysis and topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Abbreviations <a name=\"handling-abbreviations\"></a>\n",
    "\n",
    "Handling abbreviations is important for text normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample text with abbreviations\n",
    "text_with_abbrev = \"I'm going to the gym ASAP\"\n",
    "\n",
    "# A simple regex to expand common abbreviations\n",
    "abbreviations = {\n",
    "    \"ASAP\": \"as soon as possible\",\n",
    "    \"I'm\": \"I am\"\n",
    "}\n",
    "\n",
    "\n",
    "# Expanding abbreviations\n",
    "for abbrev, full_form in abbreviations.items():\n",
    "    text_with_abbrev = re.sub(r'\\b' + abbrev + r'\\b', full_form, text_with_abbrev)\n",
    "\n",
    "print(\"Expanded Text:\", text_with_abbrev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**  \n",
    "Expanding abbreviations helps in normalizing text for better understanding and processing in NLP applications, which can reduce ambiguity and improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Text Handling <a name=\"advanced-text-handling\"></a>\n",
    "\n",
    "### Multilingual Processing <a name=\"multilingual-processing\"></a>\n",
    "\n",
    "Multilingual processing is crucial for applications that involve multiple languages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of multilingual sentiment analysis\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# Sample multilingual text\n",
    "multilingual_text = \"I love programming! أحب البرمجة!\"\n",
    "\n",
    "# Sentiment analysis\n",
    "sentiment_result = sentiment_pipeline(multilingual_text)\n",
    "print(\"Sentiment Analysis Result:\", sentiment_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**  \n",
    "The Hugging Face Transformers library provides powerful tools for sentiment analysis across languages, making it easier to handle multilingual datasets. This is essential for global applications where users communicate in different languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis Preprocessing <a name=\"sentiment-analysis-preprocessing\"></a>\n",
    "\n",
    "Preprocessing text for sentiment analysis often involves cleaning and normalizing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for sentiment analysis\n",
    "def preprocess_for_sentiment(text):\n",
    "    # Remove special characters and numbers\n",
    "    cleaned_text = re.sub(r'[^A-Za-z0-9أ-ي ]+', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "cleaned_text = preprocess_for_sentiment(multilingual_text)\n",
    "print(\"Cleaned Text for Sentiment Analysis:\", cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**  \n",
    "Cleaning text is a crucial step in preparing data for analysis, as it helps eliminate noise and makes the data more uniform, which can lead to better model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion <a name=\"conclusion\"></a>\n",
    "\n",
    "In this notebook, we discussed various preprocessing techniques for both Arabic and English languages. We explored diacritization, morphological analysis, stemming, lemmatization, and more. Each technique is essential for improving the performance of NLP models. The choice of techniques aligns with the linguistic characteristics of the languages involved and the specific tasks we aim to accomplish.\n",
    "\n",
    "This notebook serves as a foundation for further exploration and experimentation in the field of NLP. Feel free to expand upon these techniques or apply them to your own datasets!\n",
    "```\n",
    "\n",
    "### Notes for Implementation:\n",
    "- Make sure to run each code cell sequentially in a Jupyter Notebook to see the outputs.\n",
    "- The sample texts can be modified to cover different aspects of Arabic and English language processing.\n",
    "- Ensure that you have the necessary libraries installed in your Python environment.\n",
    "\n",
    "This structured notebook provides a comprehensive overview of NLP preprocessing techniques for both Arabic and English languages, incorporating language-specific and advanced processing techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
